{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcc88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "|arrival|day|departure|    id|station_code|       station_name|          train_name|train_number|\n",
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "|   None|  1| 07:55:00|302214|          FM|KACHEGUDA FALAKNUMA|Falaknuma Lingamp...|       47154|\n",
      "|   None|  1| 18:55:00|281458|         TCR|            THRISUR|Thrissur Guruvayu...|       56044|\n",
      "|   None|  1| 15:05:00|309335|         PBR|          PORBANDAR|Porbandar Muzaffa...|       19269|\n",
      "|   None|  1| 13:30:00|283774|           R|          RAIPUR JN|  RAIPUR ITWARI PASS|       58205|\n",
      "+-------+---+---------+------+------------+-------------------+--------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.option(\"multiline\",\"true\").json(\"train_schedules.json\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c7ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|train_number|station_code|departure|\n",
      "+------------+------------+---------+\n",
      "|       47154|          FM| 07:55:00|\n",
      "|       56044|         TCR| 18:55:00|\n",
      "|       19269|         PBR| 15:05:00|\n",
      "|       58205|           R| 13:30:00|\n",
      "+------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"train_number\", \"station_code\", \"departure\",).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a3170545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|train_number|station_code|departure|\n",
      "+------------+------------+---------+\n",
      "|       47154|          FM| 07:55:00|\n",
      "|       56044|         TCR| 18:55:00|\n",
      "|       19269|         PBR| 15:05:00|\n",
      "|       58205|           R| 13:30:00|\n",
      "+------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.train_number, df.station_code, df.departure).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062a9ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+\n",
      "|train_number|station_code|station_code|\n",
      "+------------+------------+------------+\n",
      "|       47154|          FM|          FM|\n",
      "|       56044|         TCR|         TCR|\n",
      "|       19269|         PBR|         PBR|\n",
      "|       58205|           R|           R|\n",
      "+------------+------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"train_number\"), col(\"station_code\"), col(\"station_code\")) \\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c4cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+\n",
      "|train_number|station_code|station_code|\n",
      "+------------+------------+------------+\n",
      "|       47154|          FM|          FM|\n",
      "|       56044|         TCR|         TCR|\n",
      "|       19269|         PBR|         PBR|\n",
      "|       58205|           R|           R|\n",
      "+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the same query using SQL\n",
    "df.createOrReplaceTempView(\"schedules\")\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code, station_code\n",
    "FROM schedules\n",
    "LIMIT 4\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d202879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/jovyan/work/spark-warehouse')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37f430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3686bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cc1562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='schedules', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d98c57dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='schedules', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('global_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('create database freblogg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e941c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |schedules|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables from default').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ea2a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |schedules|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables from global_temp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180289c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- departure: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- train_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c913829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arrival',\n",
       " 'day',\n",
       " 'departure',\n",
       " 'id',\n",
       " 'station_code',\n",
       " 'station_name',\n",
       " 'train_name',\n",
       " 'train_number']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63a4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      station_name|count|\n",
      "+------------------+-----+\n",
      "|                  |    2|\n",
      "|A-CABIN BONDAMUNDA|   48|\n",
      "|             ABADA|  182|\n",
      "|          ABHAIPUR|   56|\n",
      "|  ABHAYAPURI ASSAM|   32|\n",
      "|          ABJUGANJ|   14|\n",
      "+------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"station_name\").count().orderBy(\"station_name\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fb3ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrival', 'string'),\n",
       " ('day', 'bigint'),\n",
       " ('departure', 'string'),\n",
       " ('id', 'bigint'),\n",
       " ('station_code', 'string'),\n",
       " ('station_name', 'string'),\n",
       " ('train_name', 'string'),\n",
       " ('train_number', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37e6d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.withColumn(\"departure\",to_timestamp(\"departure\"))\n",
    "df= df.withColumn(\"arrival\",to_timestamp(\"arrival\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6a94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|     arrival|   string|   null|\n",
      "|         day|   bigint|   null|\n",
      "|   departure|   string|   null|\n",
      "|          id|   bigint|   null|\n",
      "|station_code|   string|   null|\n",
      "|station_name|   string|   null|\n",
      "|  train_name|   string|   null|\n",
      "|train_number|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary table called schedules\n",
    "df.createOrReplaceTempView(\"schedules\")\n",
    "spark.sql(\"DESCRIBE schedules\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80f2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------------+---------+----------+----------------+\n",
      "|train_number|station_code|     station_name|departure|row_number|upcoming_arrival|\n",
      "+------------+------------+-----------------+---------+----------+----------------+\n",
      "|       12301|         HWH|        HOWRAH JN| 16:55:00|         1|        16:58:00|\n",
      "|       12301|         LLH|           LILUAH| 16:58:00|         2|        17:00:00|\n",
      "|       12301|         BEQ|            BELUR| 17:00:00|         3|        17:01:00|\n",
      "|       12301|         BLY|            BALLY| 17:01:00|         4|        17:03:00|\n",
      "|       12301|         BZL|        BELANAGAR| 17:03:00|         5|        17:05:00|\n",
      "|       12301|        DKAE|          DANKUNI| 17:05:00|         6|        17:07:00|\n",
      "|       12301|        GBRA|            GOBRA| 17:07:00|         7|        17:10:00|\n",
      "|       12301|         JOX|       JANAI ROAD| 17:10:00|         8|        17:11:00|\n",
      "|       12301|        BPAE|         BEGUMPUR| 17:11:00|         9|        17:14:00|\n",
      "|       12301|        BRPA|        BARUIPARA| 17:14:00|        10|        17:16:00|\n",
      "|       12301|         MBE|MIRZAPUR BANKIPUR| 17:16:00|        11|        17:17:00|\n",
      "|       12301|        BLAE|      BALARAMBATI| 17:17:00|        12|        17:19:00|\n",
      "|       12301|         KQU|       KAMARKUNDU| 17:19:00|        13|        17:21:00|\n",
      "|       12301|        MDSE|   MADHU SUDANPUR| 17:21:00|        14|        17:24:00|\n",
      "|       12301|        CDAE|       CHANDANPUR| 17:24:00|        15|        17:27:00|\n",
      "|       12301|         PBZ|        PORABAZAR| 17:27:00|        16|        17:28:00|\n",
      "|       12301|        BMAE|          BELMURI| 17:28:00|        17|        17:30:00|\n",
      "|       12301|        DNHL|      DHANIAKHALI| 17:30:00|        18|        17:32:00|\n",
      "|       12301|        SHBC|      SIBAICHANDI| 17:32:00|        19|        17:35:00|\n",
      "|       12301|         HIH|         HAJIGARH| 17:35:00|        20|        17:36:00|\n",
      "+------------+------------+-----------------+---------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding row numbers\n",
    "# Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , station_name, departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number, \n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM    schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e15671df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------------------------+-------------------+\n",
      "|train_number|station_code|unix_timestamp(departure, Yyyy-mm-dd)|   upcoming_arrival|\n",
      "+------------+------------+-------------------------------------+-------------------+\n",
      "|       12301|         HWH|                           1619436300|2021-04-26 16:58:00|\n",
      "|       12301|         LLH|                           1619436480|2021-04-26 17:00:00|\n",
      "|       12301|         BEQ|                           1619436600|2021-04-26 17:01:00|\n",
      "|       12301|         BLY|                           1619436660|2021-04-26 17:03:00|\n",
      "+------------+------------+-------------------------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding row numbers\n",
    "# Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , (UNIX_TIMESTAMP(departure, 'Yyyy-mm-dd')), \n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM    schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0786a1",
   "metadata": {},
   "source": [
    "### Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "36b6a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+\n",
      "|train_number|station_code|departure|row_number|\n",
      "+------------+------------+---------+----------+\n",
      "|       12301|         HWH| 16:55:00|         1|\n",
      "|       12301|         LLH| 16:58:00|         2|\n",
      "|       12301|         BEQ| 17:00:00|         3|\n",
      "|       12301|         BLY| 17:01:00|         4|\n",
      "|       12301|         BZL| 17:03:00|         5|\n",
      "+------------+------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVER Clause: Adding row numbers\n",
    "df.createOrReplaceTempView(\"schedules\")\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number\n",
    "        FROM schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "99513a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+----------------+\n",
      "|train_number|station_code|departure|row_number|upcoming_arrival|\n",
      "+------------+------------+---------+----------+----------------+\n",
      "|       12301|         HWH| 16:55:00|         1|        16:58:00|\n",
      "|       12301|         LLH| 16:58:00|         2|        17:00:00|\n",
      "|       12301|         BEQ| 17:00:00|         3|        17:01:00|\n",
      "|       12301|         BLY| 17:01:00|         4|        17:03:00|\n",
      "|       12301|         BZL| 17:03:00|         5|        17:05:00|\n",
      "+------------+------------+---------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEAD Clause: Upcoming arrival time\n",
    "query= \"\"\"\n",
    "SELECT train_number, station_code , departure, ROW_NUMBER() OVER (ORDER BY train_number) AS row_number,\n",
    "        LEAD(departure, 1) OVER (ORDER BY train_number) AS upcoming_arrival\n",
    "        FROM schedules\n",
    "        WHERE train_number= 12301\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9991d",
   "metadata": {},
   "source": [
    "## Doing Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c161137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "credit= spark.read.csv('./data/german_credit.csv', sep= ',', header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c5db8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|   Account Balance|   No of dependents|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|              1000|               1000|\n",
      "|   mean|             2.577|              1.155|\n",
      "| stddev|1.2576377271108936|0.36208577175319395|\n",
      "|    min|                 1|                  1|\n",
      "|    max|                 4|                  2|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics \n",
    "num_cols = ['Account Balance','No of dependents']\n",
    "credit.select(num_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc2d4485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|skewness(Age (years))|kurtosis(Age (years))|\n",
      "+---------------------+---------------------+\n",
      "|   1.0231743160548064|   0.6114371688367672|\n",
      "+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, skewness, kurtosis\n",
    "credit.select(skewness(\"Age (years)\"),kurtosis(\"Age (years)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b05c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Test\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test\n",
    "# Kolmogrov Smirnov Test\n",
    "# Correlation\n",
    "# Multivariate Gaussian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
